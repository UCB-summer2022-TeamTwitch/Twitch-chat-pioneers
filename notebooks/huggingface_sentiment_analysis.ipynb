{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "## using RoBERTa trained on twitter sentiment analysis: \n",
    "## https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'Positive', 'score': 0.985775887966156},\n",
       " {'label': 'Neutral', 'score': 0.9331138730049133},\n",
       " {'label': 'Positive', 'score': 0.667307436466217},\n",
       " {'label': 'Negative', 'score': 0.8625349402427673},\n",
       " {'label': 'Positive', 'score': 0.45673370361328125},\n",
       " {'label': 'Neutral', 'score': 0.6843876838684082},\n",
       " {'label': 'Negative', 'score': 0.7978640198707581},\n",
       " {'label': 'Neutral', 'score': 0.5491257905960083}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier([\"We are very happy to show you the ü§ó Transformers library.\", \n",
    "            \"Now Playing: JAI - VIP Mix by Finesu -> ***\",\n",
    "            \"I kind of like you\", \n",
    "           \"french people yikes\", \n",
    "           \"LESGOOOOO\", \n",
    "           \"POGGERS we we\",\n",
    "           \"this is WICKED\",\n",
    "           \"Leather Daddy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "text_df = pd.read_csv('../data/Fact Tables/EmoteFactTable.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Emote</th>\n",
       "      <th>Definition 1</th>\n",
       "      <th>Racism Implication?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Kappa</td>\n",
       "      <td>Kappa is a Twitch emote that is generally used...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>LUL</td>\n",
       "      <td>The acronym LUL stands for ‚ÄúLame Uncomfortable...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>TriHard</td>\n",
       "      <td>The TriHard Twitch emote was created to be use...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>PogChamp</td>\n",
       "      <td>The PogChamp emote is used to express surprise...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Pog</td>\n",
       "      <td>The PogChamp emote is used to express surprise...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>64</td>\n",
       "      <td>3Head</td>\n",
       "      <td>Used when someone says something cheesy, such ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>65</td>\n",
       "      <td>YEP</td>\n",
       "      <td>An insincere smile that many people use in sit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>66</td>\n",
       "      <td>Pepega</td>\n",
       "      <td>It is based on the Pepe the Frog character and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>67</td>\n",
       "      <td>KKona</td>\n",
       "      <td>The KKona Twitch emote is most commonly used a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>68</td>\n",
       "      <td>FeelsBirthdayMan</td>\n",
       "      <td>The FeelsBirthdayMan emote is used when celebr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID             Emote                                       Definition 1  \\\n",
       "0    1             Kappa  Kappa is a Twitch emote that is generally used...   \n",
       "1    2               LUL  The acronym LUL stands for ‚ÄúLame Uncomfortable...   \n",
       "2    3           TriHard  The TriHard Twitch emote was created to be use...   \n",
       "3    4          PogChamp  The PogChamp emote is used to express surprise...   \n",
       "4    5               Pog  The PogChamp emote is used to express surprise...   \n",
       "..  ..               ...                                                ...   \n",
       "63  64             3Head  Used when someone says something cheesy, such ...   \n",
       "64  65               YEP  An insincere smile that many people use in sit...   \n",
       "65  66            Pepega  It is based on the Pepe the Frog character and...   \n",
       "66  67             KKona  The KKona Twitch emote is most commonly used a...   \n",
       "67  68  FeelsBirthdayMan  The FeelsBirthdayMan emote is used when celebr...   \n",
       "\n",
       "    Racism Implication?  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     1  \n",
       "3                     0  \n",
       "4                     0  \n",
       "..                  ...  \n",
       "63                    0  \n",
       "64                    0  \n",
       "65                    0  \n",
       "66                    0  \n",
       "67                    0  \n",
       "\n",
       "[68 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitch_chats = text_df['Definition 1'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kappa is a Twitch emote that is generally used to relay sarcasm or as an ‚Äúeye-roll‚Äù response to something the Twitch streamer says or does on screen. Known as a ‚Äútroll‚Äù emote, Kappa is often spammed in chat when someone is sarcastic or checking to see if they have the Golden Kappa.',\n",
       " 'The acronym LUL stands for ‚ÄúLame Uncomfortable Laugh.‚Äù The LUL Twitch emote is used to express laughter and joy and depicts the late streamer, TotalBuscuit. It is often used on Twitch in the same way that you would use a laugh emote when messaging friends on other platforms.',\n",
       " 'The TriHard Twitch emote was created to be used when something exciting is happening on the screen or to help generate hype in a channel. It is the image of the streamer Trihex smiling. Use it when you‚Äôre hyped or when the streamer does something exciting.',\n",
       " 'The PogChamp emote is used to express surprise or excitement',\n",
       " 'The PogChamp emote is used to express surprise or excitement',\n",
       " 'The PogChamp emote is used to express surprise or excitement',\n",
       " 'MonkaS is an emote that is often used in Twitch chat when the streamer is going through intense or stressful gameplay moments. It is often spammed to express anxiety when the streamer is upset or worried.',\n",
       " 'VoHiYo is known by Twitch viewers as the ‚ÄúWeeb‚Äù (Weeaboo) emote. VoHiYo has often been used to either refer to someone as a weeb or in reference to anime in general. Others use it to say ‚ÄúHello‚Äù as it was originally intended or at the end of a technical explanation.',\n",
       " 'The emote is used as a laugh emote on Twitch.',\n",
       " 'The PepeLaugh emote is often accompanied by the phrase ‚ÄúHere it comes.‚Äù It is used as a laugh emote similar to LUL or the crying laughter emoji due to the image‚Äôs teary eyes.',\n",
       " 'The\\xa04Head emote is used in response to jokes or memes. It can be used in both a sincere and in a sarcastic way and is often used after someone uses a dad joke or says something cheesy. People should be able to tell its usage with context.',\n",
       " 'The 5Head emote is used when the chat, or viewers of a Twitch channel, want to tell the streamer that his statement or gameplay is very smart.',\n",
       " 'The CmonBruh emote is used as a look of confusion or as a response to racist comments.\\xa0It is often used when someone in Twitch chat says something controversial or confusing.',\n",
       " 'The term is often used when boobs are showcased on the screen or as a way to call out or mock people who are hornyposting.',\n",
       " 'The Twitch heart emote is a ‚ÄúTwitch purple‚Äù colored heart. It is used to express love, happiness, unity, gratitude, or empathy.',\n",
       " 'It is commonly used on Twitch to express cringe or nervous laughter when something strangely weird happens on stream or in Twitch chat.',\n",
       " 'The SourPLS emote is used on Twitch to represent dancing or celebration. It is usually posted when someone is excited or happy about an event. It is also used as a hype emote. The happiness and joy presented in the emote are contagious.',\n",
       " 'Often spammed when a streamer finds themself in an intense situation. It can also be used as mock worry.',\n",
       " 'This emote is used in Twitch chat when the streamer chooses to build up suspense or there is a feeling of anticipation in the chat.',\n",
       " '4Weird is used when something strange is happening that causes you to feel weird or become weirded out. It is typically used when someone says something that seems odd, stupid, or disturbing.',\n",
       " 'The Sadge meaning is no different. It can be used when another member of the community types that a loved one died yesterday or it can be used at the end of a statement that is a bit cringe mixed with an ironic sense of humor.',\n",
       " 'The gachiGASM emote is used when someone is intensely satisfied or pleased with an event on stream. It is also used as a meme reaction to attraction or suggestive content.',\n",
       " 'The PepeHands emote is used on Twitch when a user is trying to signify sadness (or pretending to be sad) or when the streamer seems upset about something. It is used in the same ways that a sad emote can be used.',\n",
       " 'The Jebaited emote is used when people are baiting for gaming tips or other information.\\xa0It is also used after a streamer has been Rick-Rolled or when someone trolls the streamer into an obvious joke.',\n",
       " 'It is used when a streamer does something cool or exciting.',\n",
       " 'The MingLee emote is used on Twitch to insinuate laughter or general sassiness. It can also be used in the same way that a general smiling or laughing emote can be used.',\n",
       " 'The WeirdChamp emote is used when someone is disappointed or feels let down, usually from something the streamer has said or done. It is also used when somebody says something that seems unbelievable.',\n",
       " 'Used when someone is trying to convey anxiety, horror, or shock due to something that is happening on stream or in chat.',\n",
       " 'Is used to identify oneself as either a part of the LGBTQ+ community or in support of it.',\n",
       " 'This means they are upset, frustrated, or bitter about an event that happened on stream, in the chat, or even in a real-life situation.',\n",
       " 'It is generally used when someone surrenders after an argument or sees that the streamer has been beaten when it comes to strategy.',\n",
       " 'Is used on Twitch to ‚Äúbless a stream‚Äù in a tongue-in-cheek way or along with a religious reference.',\n",
       " 'Is used on Twitch when something on the stream is going terribly wrong or if a chatter tells a story about a rough event in their real-life or in their own gameplay. It is the perfect emote to use when things just seem to not be going your way.',\n",
       " 'Is used to express sadness or disappointment, either when the user is truly sad or meming.',\n",
       " 'Used on Twitch when someone is being obnoxious, mean, or speaking in all caps.',\n",
       " 'Is used on Twitch if a channel is suspected of view botting or bringing in fake followers to the channel.',\n",
       " 'Is used when someone is expressing that they are thinking about something or contemplating a decision.',\n",
       " 'Is used when someone fails at a task or fails in general. It is often used when the streamer makes a mistake in the game that results in a death or having to redo a portion of the work.',\n",
       " 'Is used on Twitch as a greeting to say ‚Äúhello‚Äù or ‚ÄúHey Guys.‚Äù Many people use it when they first arrive at a channel to greet the streamer and other viewers in the chat.',\n",
       " 'Is typically used on Twitch when a streamer or chat user is going on about the double standards in society, getting special favors due to their gender, or acting like a simp. It is typically seen as an anti-sjw emote.',\n",
       " 'It is used when someone is very happy with something they have just witnessed on the stream.',\n",
       " 'It is generally used in relation to talking about Twitch, as it is the image of the PR director of Twitch, a man named Chase. It can also be used as a general smiling emote.',\n",
       " 'Is used on Twitch to state that something is hot (really, really cool). You will often see it spammed as part of a hype situation along with other hype emotes.',\n",
       " 'Is used when something sad or discouraging happens or if you are disappointed or failed at a small task.',\n",
       " 'It is used when something good is happening on stream or when your opponent fails.',\n",
       " 'The Kreygasm Twitch emote is used as a shocked, surprised, or as an excited expression, such as ‚ÄúWow‚Äù or ‚ÄúHoly Crap.‚Äù It is also used when something suggestive is said on stream or in chat. It is typically used as a reaction to amazing gameplay.',\n",
       " 'The SwiftRage Twitch emote is often used when someone is raging (either from excitement or anger). Viewers will spam it when something frustrating happens on the stream. It is seen as a passionate rage, rather than a tantrum.',\n",
       " 'PogU is generally used when a streamer does something cool or exciting, like beat a hard boss in a game. It can also be used to express surprise or shock.',\n",
       " 'Is used when people reference or create conspiracy theories or make ‚ÄúIlluminati signs.‚Äù It can also be used when something seems sneaky or suspicious on screen.',\n",
       " 'Is usually used on Twitch when someone is disgusted or annoyed with something the streamer or chat says or does.',\n",
       " 'Is used to mock someone who is throwing a tantrum for no reason',\n",
       " 'Is used when a server is suffering from ping or low latency.',\n",
       " 'It is often used in waves of nostalgia or when someone has a fantastic idea.',\n",
       " 'The ResidentSleeper emote is often used on Twitch when a stream or topic is boring.',\n",
       " 'The Seemsgood emote is used on Twitch as a thumbs up emote either when someone does something good, or in a sarcastic way.',\n",
       " 'It is used when someone is expressing that they are under a lot of pressure or experiencing anxiety.',\n",
       " 'The Wutface Twitch emote is used to express disbelief or shock. Most often, it is spammed when something surprising happens on stream.\\xa0',\n",
       " 'The AYAYA emote is typically used on Twitch as a sign of excitement or pleasure.',\n",
       " 'TheThing emote is used on Twitch when someone wants to make a funny face',\n",
       " 'The Keepo emote is used on Twitch in the same way that Kappa is used, to indicate sarcasm.',\n",
       " 'The BrokeBack emote is used on Twitch when someone does something dopey or if the streamer has a stroke of dumb luck.',\n",
       " 'This emote is based on the peepoHappy emote and expresses an intenisfied happiness.',\n",
       " 'The CoolStoryBob Twitch emote is used in the celebration of Bob Ross or when someone tells a ‚Äúcool story.‚Äù',\n",
       " 'Used when someone says something cheesy, such as a Dad joke.',\n",
       " 'An insincere smile that many people use in situations where they want to appear friendly but not be overly social.',\n",
       " 'It is based on the Pepe the Frog character and is often spammed ironically when someone is perceived as stupid or as having a mental disability. It can also be used when someone does something odd or out of place on the platform.',\n",
       " 'The KKona Twitch emote is most commonly used as slang for ‚ÄúAmerican‚Äù or ‚ÄúRedneck‚Äù when someone does something stereotypically American.\\xa0It can often be seen spammed in a chat where the streamer frequently does things that seem over-the-top to the rest of the world.',\n",
       " 'The FeelsBirthdayMan emote is used when celebrating someone‚Äôs birthday or when announcing your own birthday. It is also sometimes used in mock celebration.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitch_chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_polarity = classifier(twitch_chats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: Neutral, with score: 0.8278\n",
      "label: Positive, with score: 0.5579\n",
      "label: Positive, with score: 0.9341\n",
      "label: Positive, with score: 0.6078\n",
      "label: Positive, with score: 0.6078\n",
      "label: Positive, with score: 0.6078\n",
      "label: Negative, with score: 0.5644\n",
      "label: Neutral, with score: 0.8326\n",
      "label: Neutral, with score: 0.8536\n",
      "label: Neutral, with score: 0.6606\n",
      "label: Neutral, with score: 0.7448\n",
      "label: Positive, with score: 0.5979\n",
      "label: Negative, with score: 0.669\n",
      "label: Neutral, with score: 0.594\n",
      "label: Positive, with score: 0.8056\n",
      "label: Neutral, with score: 0.5911\n",
      "label: Positive, with score: 0.9542\n",
      "label: Negative, with score: 0.6981\n",
      "label: Neutral, with score: 0.6951\n",
      "label: Negative, with score: 0.6514\n",
      "label: Negative, with score: 0.639\n",
      "label: Positive, with score: 0.6598\n",
      "label: Negative, with score: 0.6499\n",
      "label: Neutral, with score: 0.4991\n",
      "label: Positive, with score: 0.8706\n",
      "label: Neutral, with score: 0.7582\n",
      "label: Negative, with score: 0.7176\n",
      "label: Neutral, with score: 0.6087\n",
      "label: Neutral, with score: 0.8956\n",
      "label: Negative, with score: 0.7759\n",
      "label: Neutral, with score: 0.5061\n",
      "label: Neutral, with score: 0.81\n",
      "label: Neutral, with score: 0.4029\n",
      "label: Negative, with score: 0.7282\n",
      "label: Negative, with score: 0.6362\n",
      "label: Negative, with score: 0.5889\n",
      "label: Neutral, with score: 0.918\n",
      "label: Negative, with score: 0.8297\n",
      "label: Neutral, with score: 0.5679\n",
      "label: Negative, with score: 0.6123\n",
      "label: Positive, with score: 0.8923\n",
      "label: Neutral, with score: 0.763\n",
      "label: Positive, with score: 0.6878\n",
      "label: Negative, with score: 0.8081\n",
      "label: Positive, with score: 0.5105\n",
      "label: Positive, with score: 0.8906\n",
      "label: Neutral, with score: 0.48\n",
      "label: Positive, with score: 0.8473\n",
      "label: Neutral, with score: 0.6634\n",
      "label: Negative, with score: 0.7034\n",
      "label: Negative, with score: 0.678\n",
      "label: Neutral, with score: 0.4886\n",
      "label: Positive, with score: 0.9084\n",
      "label: Neutral, with score: 0.5474\n",
      "label: Positive, with score: 0.7629\n",
      "label: Negative, with score: 0.5707\n",
      "label: Neutral, with score: 0.718\n",
      "label: Positive, with score: 0.8368\n",
      "label: Neutral, with score: 0.6904\n",
      "label: Neutral, with score: 0.7932\n",
      "label: Negative, with score: 0.468\n",
      "label: Positive, with score: 0.6717\n",
      "label: Positive, with score: 0.7797\n",
      "label: Neutral, with score: 0.6485\n",
      "label: Neutral, with score: 0.6841\n",
      "label: Negative, with score: 0.6906\n",
      "label: Neutral, with score: 0.6086\n",
      "label: Positive, with score: 0.5009\n"
     ]
    }
   ],
   "source": [
    "list_polarity_label = []\n",
    "for result in list_polarity:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")\n",
    "    list_polarity_label.append(result['label'].upper())\n",
    "    #     list_polarity_label.append(round(result['score'], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['BERT_polarity'] = list_polarity_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.to_csv('../data/Fact tables/EmoteFactTable_BERT.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/processed/BERT_training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Kappa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>LUL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>TriHard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>PogChamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Pog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1</td>\n",
       "      <td>3Head</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1</td>\n",
       "      <td>YEP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0</td>\n",
       "      <td>Pepega</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1</td>\n",
       "      <td>KKona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2</td>\n",
       "      <td>FeelsBirthdayMan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    label              text\n",
       "0       1             Kappa\n",
       "1       2               LUL\n",
       "2       2           TriHard\n",
       "3       2          PogChamp\n",
       "4       2               Pog\n",
       "..    ...               ...\n",
       "63      1             3Head\n",
       "64      1               YEP\n",
       "65      0            Pepega\n",
       "66      1             KKona\n",
       "67      2  FeelsBirthdayMan\n",
       "\n",
       "[68 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset \n",
    "bert_dataset = Dataset.from_pandas(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text'],\n",
       "    num_rows: 68\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset yelp_review_full (/Users/Vaibhav_Beohar/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c99ccd119b44a52bb409987640b31a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\\\nThe cashier took my friends\\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\\\\"serving off their orders\\\\\" when they didn\\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\\\nThe manager was rude when giving me my order. She didn\\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\\\nI\\'ve eaten at various McDonalds restaurants for over 30 years. I\\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, RobertaTokenizer\n",
    "# tokenizer = RobertaTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=4)\n",
    "    return tokenizer(examples[\"text\"], max_length=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31aa05db54f04cb582ed244ae7de7105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets_v2 = bert_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(10))\n",
    "# small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset_v2 = tokenized_datasets_v2.shuffle(seed=42).select(range(65))\n",
    "small_eval_dataset_v2 = tokenized_datasets_v2.shuffle(seed=42).select(range(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 2,\n",
       " 'text': 'gachiGASM',\n",
       " 'input_ids': [0, 571, 16940, 2],\n",
       " 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_train_dataset_v2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.\\\n",
    "    from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c8f0a3f7bc4bc68bfe28c2ef2b0026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits), # axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TFRobertaForSequenceClassification' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-d9c6f0d2fd53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msmall_train_dataset_v2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msmall_eval_dataset_v2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplace_model_on_device\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;31m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m         \u001b[0;31m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mParallelMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tie_weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TFRobertaForSequenceClassification' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset_v2,\n",
    "    eval_dataset=small_eval_dataset_v2,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 65\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 27\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target -1 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         )\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1552\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1554\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m                 if (\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2182\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast_smart_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2183\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2213\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2215\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2216\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2217\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1235\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"single_label_classification\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multi_label_classification\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1164\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2994\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2995\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2996\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Target -1 is out of bounds."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function TensorflowDatasetMixin.to_tf_dataset.<locals>.fetch_function at 0x7fec6300f320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function TensorflowDatasetMixin.to_tf_dataset.<locals>.fetch_function at 0x7fec6300f320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function TensorflowDatasetMixin.to_tf_dataset.<locals>.ensure_shapes at 0x7fec62647320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function TensorflowDatasetMixin.to_tf_dataset.<locals>.ensure_shapes at 0x7fec62647320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function TensorflowDatasetMixin.to_tf_dataset.<locals>.split_features_and_labels at 0x7fec4cdfcb00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function TensorflowDatasetMixin.to_tf_dataset.<locals>.split_features_and_labels at 0x7fec4cdfcb00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function TensorflowDatasetMixin.to_tf_dataset.<locals>.fetch_function at 0x7fec4cfce290> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function TensorflowDatasetMixin.to_tf_dataset.<locals>.fetch_function at 0x7fec4cfce290> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function TensorflowDatasetMixin.to_tf_dataset.<locals>.ensure_shapes at 0x7fec4d465c20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function TensorflowDatasetMixin.to_tf_dataset.<locals>.ensure_shapes at 0x7fec4d465c20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function TensorflowDatasetMixin.to_tf_dataset.<locals>.split_features_and_labels at 0x7fec4d465710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function TensorflowDatasetMixin.to_tf_dataset.<locals>.split_features_and_labels at 0x7fec4d465710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "tf_train_dataset = small_train_dataset_v2.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "tf_validation_dataset = small_eval_dataset_v2.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Vaibhav_Beohar/Documents/VB_Mck_Docs/MIDS/W210/final_proj/Twitch-chat-pioneers/notebooks/../models/twitch-roberta-base-sentiment-latest is already a clone of https://huggingface.co/veb/twitch-roberta-base-sentiment-latest. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "push_to_hub_callback = PushToHubCallback(\n",
    "    output_dir=\"../models/twitch-roberta-base-sentiment-latest\", tokenizer=tokenizer, hub_model_id=\"veb/twitch-roberta-base-sentiment-latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fec62648950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fec62648950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFRobertaForSequenceClassification.call of <transformers.models.roberta.modeling_tf_roberta.TFRobertaForSequenceClassification object at 0x7fec4d4611d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFRobertaForSequenceClassification.call of <transformers.models.roberta.modeling_tf_roberta.TFRobertaForSequenceClassification object at 0x7fec4d4611d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFRobertaMainLayer.call of <transformers.models.roberta.modeling_tf_roberta.TFRobertaMainLayer object at 0x7fec81f78510>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFRobertaMainLayer.call of <transformers.models.roberta.modeling_tf_roberta.TFRobertaMainLayer object at 0x7fec81f78510>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFRobertaEmbeddings.call of <transformers.models.roberta.modeling_tf_roberta.TFRobertaEmbeddings object at 0x7fec81f66490>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFRobertaEmbeddings.call of <transformers.models.roberta.modeling_tf_roberta.TFRobertaEmbeddings object at 0x7fec81f66490>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFRobertaEncoder.call of <transformers.models.roberta.modeling_tf_roberta.TFRobertaEncoder object at 0x7fec630088d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFRobertaEncoder.call of <transformers.models.roberta.modeling_tf_roberta.TFRobertaEncoder object at 0x7fec630088d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFRobertaLayer.call of <transformers.models.roberta.modeling_tf_roberta.TFRobertaLayer object at 0x7fec81f66bd0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFRobertaLayer.call of <transformers.models.roberta.modeling_tf_roberta.TFRobertaLayer object at 0x7fec81f66bd0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFRobertaAttention.call of <transformers.models.roberta.modeling_tf_roberta.TFRobertaAttention object at 0x7fec4cfd5cd0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFRobertaAttention.call of <transformers.models.roberta.modeling_tf_roberta.TFRobertaAttention object at 0x7fec4cfd5cd0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFRobertaSelfAttention.call of <transformers.models.roberta.modeling_tf_roberta.TFRobertaSelfAttention object at 0x7fec73091fd0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFRobertaSelfAttention.call of <transformers.models.roberta.modeling_tf_roberta.TFRobertaSelfAttention object at 0x7fec73091fd0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFRobertaSelfOutput.call of <transformers.models.roberta.modeling_tf_roberta.TFRobertaSelfOutput object at 0x7fec81f86d10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method TFRobertaSelfOutput.call of <transformers.models.roberta.modeling_tf_roberta.TFRobertaSelfOutput object at 0x7fec81f86d10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFRobertaIntermediate.call of <transformers.models.roberta.modeling_tf_roberta.TFRobertaIntermediate object at 0x7fec63008450>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFRobertaIntermediate.call of <transformers.models.roberta.modeling_tf_roberta.TFRobertaIntermediate object at 0x7fec63008450>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFRobertaOutput.call of <transformers.models.roberta.modeling_tf_roberta.TFRobertaOutput object at 0x7fec4ce88dd0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFRobertaOutput.call of <transformers.models.roberta.modeling_tf_roberta.TFRobertaOutput object at 0x7fec4ce88dd0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFRobertaClassificationHead.call of <transformers.models.roberta.modeling_tf_roberta.TFRobertaClassificationHead object at 0x7fec4d2a09d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFRobertaClassificationHead.call of <transformers.models.roberta.modeling_tf_roberta.TFRobertaClassificationHead object at 0x7fec4d2a09d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "8/8 [==============================] - ETA: 0s - loss: 1.1272 - sparse_categorical_accuracy: 0.3281WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fec214b30e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fec214b30e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (5) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 32s 3s/step - loss: 1.1272 - sparse_categorical_accuracy: 0.3281 - val_loss: 1.0190 - val_sparse_categorical_accuracy: 0.3333\n",
      "Epoch 2/3\n",
      "8/8 [==============================] - 4s 513ms/step - loss: 1.1254 - sparse_categorical_accuracy: 0.2969 - val_loss: 1.1164 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 3/3\n",
      "8/8 [==============================] - 4s 495ms/step - loss: 1.0941 - sparse_categorical_accuracy: 0.3750 - val_loss: 1.0186 - val_sparse_categorical_accuracy: 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (6) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb7329ec289443a8640c680f2ce7877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file tf_model.h5:   0%|          | 32.0k/476M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Enforcing permissions...        \n",
      "remote: Allowed refs: all        \n",
      "To https://huggingface.co/veb/twitch-roberta-base-sentiment-latest\n",
      "   959fae6..c8d9b7e  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fec4d48e690>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
    ")\n",
    "\n",
    "model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3, callbacks=push_to_hub_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 2, 'text': 'gachiGASM', 'input_ids': [0, 571, 16940, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 1, 'text': 'KKona', 'input_ids': [0, 26228, 4488, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 2, 'text': 'Pog', 'input_ids': [0, 510, 2154, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 0, 'text': 'BabyRage', 'input_ids': [0, 30047, 500, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 0, 'text': 'BrokeBack', 'input_ids': [0, 26519, 1071, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 1, 'text': 'Keepo', 'input_ids': [0, 28407, 139, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 1, 'text': 'PauseChamp', 'input_ids': [0, 48532, 43225, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 0, 'text': 'MonkaW', 'input_ids': [0, 17312, 2348, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 1, 'text': 'VoHiYo', 'input_ids': [0, 37660, 30086, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 2, 'text': 'widepeepoHappy', 'input_ids': [0, 6445, 2379, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 0, 'text': 'BibleThump', 'input_ids': [0, 387, 4748, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 2, 'text': 'CurseLit', 'input_ids': [0, 347, 33901, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 2, 'text': 'FeelsBirthdayMan', 'input_ids': [0, 29037, 2507, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 2, 'text': 'MorphinTime', 'input_ids': [0, 448, 31724, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 2, 'text': 'CoolStoryBob', 'input_ids': [0, 37739, 39630, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 1, 'text': 'GayPride', 'input_ids': [0, 43086, 21077, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 2, 'text': 'Poggers', 'input_ids': [0, 510, 2154, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 2, 'text': 'LUL', 'input_ids': [0, 574, 6597, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 2, 'text': 'COGGERS', 'input_ids': [0, 347, 10207, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 0, 'text': 'SquadW', 'input_ids': [0, 38378, 625, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 1, 'text': 'MonkaGIGA', 'input_ids': [0, 17312, 2348, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 0, 'text': 'FailFish', 'input_ids': [0, 48237, 43643, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 0, 'text': 'WeirdChamp', 'input_ids': [0, 170, 8602, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 1, 'text': '3Head', 'input_ids': [0, 246, 28873, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 0, 'text': 'Sadge', 'input_ids': [0, 37188, 1899, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 2, 'text': 'Seemsgood', 'input_ids': [0, 14696, 17752, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 1, 'text': 'TheThing', 'input_ids': [0, 133, 565, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 2, 'text': 'TriHard', 'input_ids': [0, 35587, 31574, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 1, 'text': 'Wutface', 'input_ids': [0, 771, 1182, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 0, 'text': 'PJSalt', 'input_ids': [0, 510, 37337, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 1, 'text': 'MingLee', 'input_ids': [0, 448, 154, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 1, 'text': 'NotLikeThis', 'input_ids': [0, 7199, 13721, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 1, 'text': 'Jebaited', 'input_ids': [0, 863, 3209, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 1, 'text': 'TheIlluminati', 'input_ids': [0, 133, 24873, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 1, 'text': 'haHaa', 'input_ids': [0, 1999, 725, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 1, 'text': 'PepeLaugh', 'input_ids': [0, 23029, 2379, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 1, 'text': 'BlessRNG', 'input_ids': [0, 387, 1672, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 2, 'text': 'gachiBASS', 'input_ids': [0, 571, 16940, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 2, 'text': 'SourPLS', 'input_ids': [0, 104, 2126, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 2, 'text': 'FeelsGoodMan', 'input_ids': [0, 29037, 2507, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 2, 'text': 'PogChamp', 'input_ids': [0, 510, 2154, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 0, 'text': 'SMOrc', 'input_ids': [0, 15153, 11094, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 1, 'text': 'MonkaH', 'input_ids': [0, 17312, 2348, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 1, 'text': '4Head', 'input_ids': [0, 306, 28873, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 0, 'text': 'monkaOMEGA', 'input_ids': [0, 5806, 2348, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 1, 'text': 'YEP', 'input_ids': [0, 975, 9662, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 0, 'text': 'MonkaS', 'input_ids': [0, 17312, 2348, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 1, 'text': 'HeyGuys', 'input_ids': [0, 13368, 14484, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 2, 'text': 'AYAYA', 'input_ids': [0, 2547, 2547, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 2, 'text': '5Head', 'input_ids': [0, 245, 28873, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 1, 'text': 'SabaPing', 'input_ids': [0, 104, 6412, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 1, 'text': 'PRChase', 'input_ids': [0, 4454, 4771, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 1, 'text': 'SwiftRage', 'input_ids': [0, 15417, 9417, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 0, 'text': 'Pepehands', 'input_ids': [0, 23029, 2379, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 0, 'text': '4Weird', 'input_ids': [0, 306, 170, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 0, 'text': 'MrDestructoid', 'input_ids': [0, 10980, 42551, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 1, 'text': 'Kappa', 'input_ids': [0, 530, 22181, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 2, 'text': 'PogU', 'input_ids': [0, 510, 2154, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 0, 'text': 'DansGame', 'input_ids': [0, 495, 1253, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 2, 'text': 'Kreygasm', 'input_ids': [0, 530, 5460, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 0, 'text': 'CmonBruh', 'input_ids': [0, 347, 5806, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 1, 'text': 'ResidentSleeper', 'input_ids': [0, 20028, 8009, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 0, 'text': 'FeelsBadMan', 'input_ids': [0, 29037, 2507, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 2, 'text': '<3', 'input_ids': [0, 41552, 246, 2], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'label': 1, 'text': 'Booba', 'input_ids': [0, 18935, 19614, 2], 'attention_mask': [1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "for i in small_train_dataset_v2:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e3170af3114fcfabb6372ea402baf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/476M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing RobertaForTokenClassification.\n",
      "\n",
      "All the weights of RobertaForTokenClassification were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForTokenClassification for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at veb/twitch-roberta-base-sentiment-latest.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17fcc816f4c34d3ea3cd9f584b6ef1cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11cc932f950c450696204d8e9fd6a84f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/780k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2012756a6ed54acfb3f137cefd3aa4a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166943ee842842b1bca0b92bae144dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.01M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0f407a32004c808204df16ddf0569f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoModelForTokenClassification.from_pretrained('veb/twitch-roberta-base-sentiment-latest', from_tf=True)\n",
    "classifier = pipeline(\"sentiment-analysis\", model='veb/twitch-roberta-base-sentiment-latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'Neutral', 'score': 0.38475385308265686}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(['i hate you'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Kappa is a Twitch emote that is generally used to ,  label: Neutral, with score: 0.3971\n",
      "Text: The acronym LUL stands for ‚ÄúLame Uncomfortable Lau,  label: Neutral, with score: 0.3835\n",
      "Text: The TriHard Twitch emote was created to be used wh,  label: Neutral, with score: 0.3893\n",
      "Text: The PogChamp emote is used to express surprise or ,  label: Neutral, with score: 0.3653\n",
      "Text: The PogChamp emote is used to express surprise or ,  label: Neutral, with score: 0.3653\n",
      "Text: The PogChamp emote is used to express surprise or ,  label: Neutral, with score: 0.3653\n",
      "Text: MonkaS is an emote that is often used in Twitch ch,  label: Positive, with score: 0.3964\n",
      "Text: VoHiYo is known by Twitch viewers as the ‚ÄúWeeb‚Äù (W,  label: Neutral, with score: 0.4029\n",
      "Text: The emote is used as a laugh emote on Twitch.,  label: Neutral, with score: 0.3669\n",
      "Text: The PepeLaugh emote is often accompanied by the ph,  label: Neutral, with score: 0.3705\n",
      "Text: The¬†4Head emote is used in response to jokes or me,  label: Neutral, with score: 0.3781\n",
      "Text: The 5Head emote is used when the chat, or viewers ,  label: Neutral, with score: 0.3844\n",
      "Text: The CmonBruh emote is used as a look of confusion ,  label: Neutral, with score: 0.383\n",
      "Text: The term is often used when boobs are showcased on,  label: Neutral, with score: 0.383\n",
      "Text: The Twitch heart emote is a ‚ÄúTwitch purple‚Äù colore,  label: Neutral, with score: 0.3734\n",
      "Text: It is commonly used on Twitch to express cringe or,  label: Positive, with score: 0.3749\n",
      "Text: The SourPLS emote is used on Twitch to represent d,  label: Neutral, with score: 0.3744\n",
      "Text: Often spammed when a streamer finds themself in an,  label: Positive, with score: 0.3863\n",
      "Text: This emote is used in Twitch chat when the streame,  label: Neutral, with score: 0.3805\n",
      "Text: 4Weird is used when something strange is happening,  label: Positive, with score: 0.4042\n",
      "Text: The Sadge meaning is no different. It can be used ,  label: Positive, with score: 0.3933\n",
      "Text: The gachiGASM emote is used when someone is intens,  label: Positive, with score: 0.3756\n",
      "Text: The PepeHands emote is used on Twitch when a user ,  label: Positive, with score: 0.3941\n",
      "Text: The Jebaited emote is used when people are baiting,  label: Neutral, with score: 0.3956\n",
      "Text: It is used when a streamer does something cool or ,  label: Positive, with score: 0.3627\n",
      "Text: The MingLee emote is used on Twitch to insinuate l,  label: Neutral, with score: 0.3823\n",
      "Text: The WeirdChamp emote is used when someone is disap,  label: Positive, with score: 0.3998\n",
      "Text: Used when someone is trying to convey anxiety, hor,  label: Positive, with score: 0.3765\n",
      "Text: Is used to identify oneself as either a part of th,  label: Neutral, with score: 0.3699\n",
      "Text: This means they are upset, frustrated, or bitter a,  label: Positive, with score: 0.3929\n",
      "Text: It is generally used when someone surrenders after,  label: Positive, with score: 0.3795\n",
      "Text: Is used on Twitch to ‚Äúbless a stream‚Äù in a tongue-,  label: Neutral, with score: 0.3692\n",
      "Text: Is used on Twitch when something on the stream is ,  label: Positive, with score: 0.4229\n",
      "Text: Is used to express sadness or disappointment, eith,  label: Positive, with score: 0.3916\n",
      "Text: Used on Twitch when someone is being obnoxious, me,  label: Positive, with score: 0.3696\n",
      "Text: Is used on Twitch if a channel is suspected of vie,  label: Positive, with score: 0.374\n",
      "Text: Is used when someone is expressing that they are t,  label: Positive, with score: 0.3734\n",
      "Text: Is used when someone fails at a task or fails in g,  label: Positive, with score: 0.3956\n",
      "Text: Is used on Twitch as a greeting to say ‚Äúhello‚Äù or ,  label: Neutral, with score: 0.3916\n",
      "Text: Is typically used on Twitch when a streamer or cha,  label: Positive, with score: 0.3925\n",
      "Text: It is used when someone is very happy with somethi,  label: Positive, with score: 0.3706\n",
      "Text: It is generally used in relation to talking about ,  label: Neutral, with score: 0.4008\n",
      "Text: Is used on Twitch to state that something is hot (,  label: Positive, with score: 0.3859\n",
      "Text: Is used when something sad or discouraging happens,  label: Positive, with score: 0.4027\n",
      "Text: It is used when something good is happening on str,  label: Positive, with score: 0.3691\n",
      "Text: The Kreygasm Twitch emote is used as a shocked, su,  label: Positive, with score: 0.3774\n",
      "Text: The SwiftRage Twitch emote is often used when some,  label: Positive, with score: 0.3971\n",
      "Text: PogU is generally used when a streamer does someth,  label: Positive, with score: 0.3797\n",
      "Text: Is used when people reference or create conspiracy,  label: Positive, with score: 0.3742\n",
      "Text: Is usually used on Twitch when someone is disguste,  label: Positive, with score: 0.3852\n",
      "Text: Is used to mock someone who is throwing a tantrum ,  label: Positive, with score: 0.3786\n",
      "Text: Is used when a server is suffering from ping or lo,  label: Positive, with score: 0.3698\n",
      "Text: It is often used in waves of nostalgia or when som,  label: Positive, with score: 0.3624\n",
      "Text: The ResidentSleeper emote is often used on Twitch ,  label: Neutral, with score: 0.3676\n",
      "Text: The Seemsgood emote is used on Twitch as a thumbs ,  label: Positive, with score: 0.3684\n",
      "Text: It is used when someone is expressing that they ar,  label: Positive, with score: 0.3778\n",
      "Text: The Wutface Twitch emote is used to express disbel,  label: Neutral, with score: 0.3733\n",
      "Text: The AYAYA emote is typically used on Twitch as a s,  label: Neutral, with score: 0.3667\n",
      "Text: TheThing emote is used on Twitch when someone want,  label: Neutral, with score: 0.3689\n",
      "Text: The Keepo emote is used on Twitch in the same way ,  label: Neutral, with score: 0.3775\n",
      "Text: The BrokeBack emote is used on Twitch when someone,  label: Positive, with score: 0.3712\n",
      "Text: This emote is based on the peepoHappy emote and ex,  label: Positive, with score: 0.367\n",
      "Text: The CoolStoryBob Twitch emote is used in the celeb,  label: Neutral, with score: 0.3855\n",
      "Text: Used when someone says something cheesy, such as a,  label: Neutral, with score: 0.361\n",
      "Text: An insincere smile that many people use in situati,  label: Positive, with score: 0.379\n",
      "Text: It is based on the Pepe the Frog character and is ,  label: Positive, with score: 0.3795\n",
      "Text: The KKona Twitch emote is most commonly used as sl,  label: Neutral, with score: 0.3842\n",
      "Text: The FeelsBirthdayMan emote is used when celebratin,  label: Neutral, with score: 0.3784\n"
     ]
    }
   ],
   "source": [
    "list_polarity = classifier(twitch_chats)\n",
    "list_polarity_label = []\n",
    "for t, result in zip(twitch_chats, list_polarity):\n",
    "    print(f\"Text: {t[:50]},  label: {result['label']}, with score: {round(result['score'], 4)}\")\n",
    "    list_polarity_label.append(result['label'].upper())\n",
    "    #     list_polarity_label.append(round(result['score'], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
